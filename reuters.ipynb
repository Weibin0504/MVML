{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_list = ['00','01','02','03','04','05','06','07','08','09',\\\n",
    "             '10','11','12','13','14','15','16','17','18','19','20','21']\n",
    "\n",
    "def deal_sgml(data_list):\n",
    "    con_labels = np.zeros((1,5))\n",
    "    for k in data_list:\n",
    "        f = open('F:\\\\wen\\\\yan_2_1\\\\compare\\\\reuters\\\\reut2-0'+k+'.sgm','r')\n",
    "        soup = BeautifulSoup(f,'lxml')\n",
    "        if k != '21':\n",
    "            sample = 1000\n",
    "        else:\n",
    "            sample = 578\n",
    "        labels = np.zeros((sample,5))\n",
    "        label_list = ['topics','places','people','orgs','exchanges']\n",
    "        j = 0\n",
    "        for lists in label_list:\n",
    "            i = 0\n",
    "            for label in soup.find_all(lists):\n",
    "                if label.d != None:\n",
    "                    labels[i][j] = 1\n",
    "                i += 1\n",
    "            j += 1\n",
    "        print k\n",
    "        con_labels = np.r_[con_labels,labels]\n",
    "    con_labels = np.delete(con_labels,0,axis=0)\n",
    "    return con_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_label = deal_sgml(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = np.where(all_label<1,-1,1)\n",
    "np.savetxt('F:\\\\wen\\\\yan_2_1\\\\compare\\\\reuters\\\\data\\\\label.csv',true_label,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear the title and cont to txt\n",
    "title_data = []\n",
    "cont_data = []\n",
    "def deal_sgml_text(data_list):\n",
    "    for k in data_list:\n",
    "        f = open('F:\\\\wen\\\\yan_2_1\\\\compare\\\\reuters\\\\reut2-0'+k+'.sgm','r')\n",
    "        soup = BeautifulSoup(f,'lxml')\n",
    "        '''\n",
    "        #deal with the title to text\n",
    "        for text in soup.find_all('text'):\n",
    "            if text.title != None:\n",
    "                title_data.append(text.title.string)\n",
    "            else:\n",
    "                title_data.append('None')\n",
    "                        \n",
    "        '''\n",
    "        #deal with the cont to text\n",
    "        [s.extract() for s in soup(['title','dateline','author'])] #delete some title\n",
    "        for text in soup.find_all('text'):\n",
    "            if text.type == 'brief':\n",
    "                print 'hehe'\n",
    "                cont_data.append('None')\n",
    "            elif text != None:\n",
    "                cont_data.append(str(text))\n",
    "                #print str(text)\n",
    "            else:\n",
    "                cont_data.append('None')\n",
    "        print k\n",
    "    #return cont_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal_sgml_text(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "with open('F:\\\\wen\\\\yan_2_1\\\\compare\\\\reuters\\\\data\\\\cont.txt','wb') as f:\n",
    "    for i in cont_data:\n",
    "        i = i.replace('\\n', '')\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21578"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_data)\n",
    "#print cont_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#clear the cont data\n",
    "f0 = open('F:\\\\wen\\\\yan_2_1\\\\compare\\\\reuters\\\\data\\\\cont.txt','r')\n",
    "cont = f0.readlines()\n",
    "f0.close()\n",
    "with open('F:\\\\wen\\\\yan_2_1\\\\compare\\\\reuters\\\\data\\\\cont1.txt','wb') as f:\n",
    "    for line in cont:\n",
    "        if '***Blah' in line:\n",
    "            f.write('None')\n",
    "            f.write('\\n')\n",
    "        else:\n",
    "            line = line.replace('<text>','')\n",
    "            line = line.replace('Reuter</text>','')\n",
    "            line = line.replace('REUTER</text>','')\n",
    "            f.write(line)\n",
    "            #f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the none of title and cont\n",
    "import pandas as pd\n",
    "path = \"F:\\\\wen\\\\yan_2_1\\\\compare\\\\reuters\\\\data\\\\\"\n",
    "label = pd.read_csv(path+'label.csv',names=['topics','places','people','orgs','exchanges'])\n",
    "title = pd.read_table(path+'title.txt',sep='\\n',names=['title'])\n",
    "cont = pd.read_table(path+'cont.txt',sep='\\n',names=['cont'])\n",
    "data = pd.concat([label,title,cont],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    if row['title']=='None' or row['cont']=='None':\n",
    "        data = data.drop([index])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[['topics','places','people','orgs','exchanges']].to_csv(path+'nNone_label.csv',header=False,index=False)\n",
    "with open(path+'nNone_title.txt','wb') as f:\n",
    "    for line in data['title']:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open(path+'nNone_cont.txt','wb') as f:\n",
    "    for line in data['cont']:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using gensim to computer tfidf\n",
    "from gensim import corpora,models\n",
    "from collections import defaultdict\n",
    "path = \"F:\\\\wen\\\\yan_2_1\\\\compare\\\\reuters\\\\data\\\\\"\n",
    "f = open(path+'nNone_cont.txt','r')\n",
    "read_title = f.readlines()\n",
    "f.close()\n",
    "title =[]\n",
    "for line in read_title:\n",
    "    title.append(line.strip('\\n'))\n",
    "'''    \n",
    "# 去掉停用词\n",
    "stoplist = set('FOR A OF THE AND TO IN'.split())\n",
    "texts = [[word for word in title.split() if word not in stoplist]\n",
    "         for document in title]\n",
    "         \n",
    "stoplist=set('for a of the from is an and to in'.split())\n",
    "texts = []\n",
    "for corp in corpora_documents:\n",
    "    word_list = ''\n",
    "    for word in corp.lower().split():\n",
    "        if word not in stoplist:\n",
    "            word_list = word+' '+word_list\n",
    "    texts.append(word_list)\n",
    "    \n",
    "# 去掉只出现一次的单词\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(title)   # 生成词典\n",
    "\n",
    "# 将文档存入字典，字典有很多功能，比如\n",
    "# diction.token2id 存放的是单词-id key-value对\n",
    "# diction.dfs 存放的是单词的出现频率\n",
    "dictionary.save(path+'deerwester.dict')  # store the dictionary, for future reference\n",
    "'''\n",
    "\n",
    "# 生成字典和向量语料\n",
    "corpus = [x.split() for x in title]\n",
    "dct = corpora.Dictionary(corpus)\n",
    "corpus_as_bow = [dct.doc2bow(x) for x in corpus]\n",
    "model_trained = models.TfidfModel(corpus_as_bow)\n",
    "corpus_tfidf_trained = model_trained[corpus_as_bow]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor item in corpus_tfidf_trained:\\n    print(item)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for item in corpus_tfidf_trained:\n",
    "    print(item)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained.save(path+'cont_tfidf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change gensim's tfidf to sparse matrix\n",
    "#corpus_tfidf_trained = models.load(path+'title_tfidf_model')\n",
    "from scipy.sparse import csr_matrix\n",
    "data = []\n",
    "rows = []\n",
    "cols = []\n",
    "line_count = 0\n",
    "for line in corpus_tfidf_trained:  # corpus_tfidf_trained 是之前由gensim生成的tfidf向量\n",
    "    for elem in line:\n",
    "        rows.append(line_count)\n",
    "        cols.append(elem[0])\n",
    "        data.append(elem[1])\n",
    "    line_count += 1\n",
    "tfidf_sparse_matrix = csr_matrix((data,(rows,cols))) # 稀疏向量\n",
    "#tfidf_matrix = tfidf_sparse_matrix.toarray()  # 密集向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse matrix save\n",
    "from scipy.io import mmwrite\n",
    "mmwrite(path+'sp_cont_tfidf',tfidf_sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
